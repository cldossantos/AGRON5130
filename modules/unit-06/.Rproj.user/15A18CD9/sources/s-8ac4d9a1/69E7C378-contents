---
title: "Multiple Treatment Trials"
output: 
  prettydoc::html_pretty:
    theme: cayman
runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

# Introduction
Here is our course, so far, in a nutshell:

* statistics is all about populations
* when we compare treatments, we are actually comparing populations that have been exposed to different products or management
* when we can measure every individual in that population, we can use the Z-distribution to describe their true population means and variance
* most of the time in agricultural research, however, we must use a samples (subsets from those populations) to *estimate* their population means and variance
* when we use the sample mean to estimate the population mean, we use the t-distribution to descibe the distribution of sample means around the mean of their values
* the distribution of sample means can be used to calculate the probability (the p-value) the true population mean is a hypothetical value
* in a paired t-test of two populations, we create a new population of differences, and calculate the probability its mean is zero
* proper hypotheses and alphas (maximum p-values for significance) can reduce the likelihood we conclude populations are different when they are likely the same, or the same when they are likely different

I include this brutal distillation so you can see how the course has evolved from working with complete populations to samples, from "true" or "certain" estimates of population means to estimates, from working with single populations to comparing differences between two populations.

In the last two units, we learned how to design and evaluate the results of side-by-side trials: trials in fields or parts of fields were divided into two populations that were managed with different treatments or practices.  This was a practical, powerful, jumping-off point for thinking about experiments and their analyses.

Let's face it, however: if we only compared two treatments per experiment in product testing or management trials, our knowledge would advance at a much slower pace.  So in the next three units, we will learn how to design and evaluate trials to test multiple *categorical* treatments.  By *categorical*, we mean treatments we identify by name, not quantity.  Treatments that, in general, cannot be ranked.  Hybrids are a perfect example of categorical treatments.  Herbicides, fungicides, or fertilzers that differ in brand name or chemical composition are also categorical treatments.  Comparisons of cultural practices, such as tillage systems or crop rotations are categorical treatments as well.


# Case Study
For our case study, we will look at a comparison of four hybrids from the Marin Harbur seed company.  We will compare hybrids MH-052672, MH-050877, MH-091678, and MH-032990, which are not coded by relative maturity or parent lines, but represent some of the owner's favorite Grateful Dead concerts:  
- MH-052672 has a great early disease package and is ideal for environments that have heavy morning dews
- MH-050877 is very resilient in poor fertility soils and among the last to have its leaves fire under low nitrogen conditions.  It can also grow well on everything from flatlands to mountains.  
- MH-091678 has a very strong root base that will not be shaken down by late-season wind storms
- MH-032990 offers a very rich performance and responds well to additional inputs.  (This one catches growers' eyes at every field day -- not to blow our own horn.) 



```{r}
library(tidyverse)

hybrid_data = read.csv("data/grateful_data.csv")
head(hybrid_data)
```

The hybrids were grown in a field trial with four replications as shown below.

```{r}
hybrid_data %>%
  ggplot(aes(x=col, y=row, label=Hybrid)) +
  geom_tile(fill="grey", color="black") +
  geom_text(aes(label = Hybrid))
  


```

The arrangement of treatments above is a *completely randomized design*.  This means the hybrids were assigned at random among all plots -- there was no grouping or pairing of treatments as in our side-by-side trials earlier.

This design -- to be honest -- is used more often in greenhouse or growth chamber research where the soil or growing media are more uniform.  Still, it is the most appropriate design to start with in discussing multiple treatment trials.

# The Linear Additive Model
In order to understand how we will analyze this trial, let's plot out our data.

```{r include=FALSE}
trt_means= hybrid_data %>%
  group_by(Hybrid) %>%
  summarise(Yield = mean(Yield))

data_for_model = trt_means %>%
  filter(row_number() %in% c(1,3)) %>%
  mutate(x = c(0.5, -0.5))

fit_1 = lm(x ~ Yield, data=data_for_model)

pred_x = predict(fit_1, trt_means)

trt_means_w_x = cbind(trt_means, pred_x) %>%
  as.data.frame() %>%
  mutate(Yield = Yield - mean(Yield))

trt_means_plot = trt_means_w_x %>%
  ggplot(aes(x=pred_x, y=Yield, label=Hybrid)) +
  geom_point(color = "blue", size=4) +
  geom_text(hjust=0) +
  lims(x=c(-0.65, 0.65))

trt_means_plot


```

```{r}

hybrid_rank = hybrid_data %>%
  group_by(Hybrid) %>%
  summarise(Yield = mean(Yield)) %>%
  ungroup() %>%
  arrange(Yield) %>%
  mutate(rank = row_number()) %>%
  select(-Yield)

effects_data = hybrid_data %>%
  select(-col, -row) %>%
  mutate(mu = mean(Yield)) %>%
  arrange(Hybrid) %>%
  group_by(Hybrid) %>%
  mutate(T = mean(Yield) - mu) %>%
  ungroup() %>%
  mutate(E = Yield - T - mu) %>%
  left_join(hybrid_rank)

hybrids = unique(effects_data$Hybrid)

  
effects_data %>%
  ggplot(aes(x=rank, y=Yield)) +
  geom_point() +
  geom_text(aes(x=rank-0.1, y=mu+T, label=Hybrid, hjust=1)) +
  lims(x=c(0.5, 4.5))
  

  
  
```

Our treatment means are shown below:

```{r}
effects_data %>%
  ggplot(aes(x=rank, y=T)) +
  geom_point() +
  geom_text(aes(x=rank-0.1, y=T, label=Hybrid, hjust=1)) +
  geom_text(aes(x=rank+0.1, y=T, label=round(T, 1)), hjust=0) +
  lims(x=c(0.5, 4.5))
```

We can see the plot effects as well.

```{r}
effects_data %>%
  ggplot(aes(x=rank, y=T+E)) +
  geom_point()+
  geom_text(aes(x=rank-0.1, y=T, label=Hybrid, hjust=1)) +
  geom_text(aes(x=rank+0.1, y=T+E, label=round(E, 1)), hjust=0, size=4) +
  lims(x=c(0.5, 4.5))
```

<!-- As we learned earlier, it is sometimes clearer to observe the plot effects in a residual plot: -->

<!-- ```{r} -->
<!-- effects_data %>% -->
<!--   ggplot(aes(x=rank, y=E)) + -->
<!--   geom_point(size=3, color="tomato")+ -->
<!--   geom_text(aes(x=rank-0.1, y=0.3, label=Hybrid, hjust=1)) + -->
<!--   geom_text(aes(x=rank+0.1, y=E, label=round(E, 1)), hjust=0, size=4) + -->
<!--   geom_hline(aes(yintercept = 0)) + -->
<!--   lims(x=c(0, 4.5)) -->
<!-- ``` -->

We can observe the linear model using this table:
```{r}
effects_data %>%
  select(Hybrid, mu, T, E, Yield) %>%
  mutate(mu = round(mu, 1),
         T = round(T, 1),
         E = round(E, 1)) %>%
  rename(`Yield (mu + T + E)` = Yield)
```

Just as in the side-by-side trials, our statistical test is based on the ratio of the variation among treatment means to the variation among observations within each treatment.

```{r}


library(grid)
library(pBrackets)

p = effects_data %>%
  ggplot(aes(x=rank, y=T+E)) +
  geom_point(size = 2) +
  lims(x=c(0, 6))
p +  geom_text(aes(x=1.5, y=-7), label="Does the spread of\nindividuals within a treatment\nexplain more of the variance?", 
            hjust=0, vjust=0.5, size=5) +
  geom_text(aes(x=4.4, y=0), label="Or does the\ndifference between\ntreatment means\nexplain more\nof the variance?", 
            hjust=0, vjust=0.5, size=5) 

grid.locator(unit="native") 
# 
grid.brackets(180, 300, 180, 375, h=0.05, lwd=2, col="red")
grid.brackets(473, 60, 473, 335, h=0.05, lwd=2, col="red")


```


# Analysis of Variance
Chances are if you have spent time around agronomic research you have probably heard of *ANOVA*.  

![A Nova](nova.jpg)


No, not that fine example of Detroit muscle, but a statistical test, the *Analysis of Variance (ANOVA)*.  The ANOVA test performs the comparison described above when there are more than two more populations that differ categorically in their management.  I'll admit the nature of this test was a mystery to me for years, if not decades.  As the name suggests, however, it is simply an analysis (a comparison, in fact) of the different sources of variation as outlined in our linear additive model.  An Analysis of Variance tests two hypotheses:

- Ho: There is no difference among population means.
- Ha: There is a difference among population means.

In our hybrid trial, we can be more specific:

- Ho: There is no difference in yield among populations planted with four different hybrids.
- Ha: There is a difference in yield among populations planted with four different hybrids.

But the nature of the hypotheses stays the same.

# The F statistic 

The Analysis of Variance compares the variance from the treatment effect to the variance from the error effect. It does this by dividing the variance from treatments by the variance from Error:

$$F = \frac{\sigma{^2}_{treatment}}{\sigma{^2}_{error}} $$

The $F-value$ quanitfies the ratio of treatment variance to error variance.  As the ratio of the variance from the treatment effect to the variance from the error effect increases, so does the F-statistic.  In other words, a greater F-statistic suggests a greater treatment effect -- or -- a smaller error effect.


# The ANOVA Table
At this point, it is easier to explain the  
```{r}
library(broom)
hybrid_test = tidy(aov(Yield ~ Hybrid, hybrid_data))

hybrid_test

summary(hybrid_test)
```

As we did with the t-test a couple of units ago, lets go through the ANOVA output column by column, row by row.

## Source of Variation
The furthest column to the left, *term* specifies the two effects in our linear additive model: the Hybrid and Residual effects.  As mentioned in the last chapter, the term Residual is often used to describe the "leftover" variation among observations that a model cannot explain.  In this case, it refers to the variation remaining when the Hybrid effect is accounted for.  This column is also often referred to as the *Source of Variation* column.

## Sum of Squares
Let's skip the df column for a moment to expain the column titled *sumsq* in this output.  This column lists the sums of squares associated with the Hybrid and Residual Effect.  Remember, the sum of squares is the sum of the squared differences between the individuals and the population mean.  Also, we need to calculate the sum of squares before calculating variance

The Hybrid sum of squares based on the the difference between the treatment mean and the population mean for each observation in the experiment. In the table below we have mu, the population mean, and T, the effect or difference between the treatment mean and mu.  We square T for each of the 16 observations to create a new column, T-square

```{r}
trt_ss = effects_data %>%
  select(Hybrid, mu, T) %>%
  mutate(`T-square` = T^2)

# %>% 
#   mutate(mu = round(mu, 1),
#          T = round(T, 1)) 
trt_ss

```

We then sum the squares of T to get the Hybrid sum of squares.

```{r}
sum(trt_ss$`T-square`)
```

We use a similar approach to calculate the Error (or Residual) sum of squares.  This time we square the error effect (the difference between the observed value and the treatment mean) for each observation in the trial.  

```{r}
err_ss = effects_data %>%
  select(Hybrid, mu, E) %>%
  mutate(`E-square` = E^2)

err_ss
```

We again sum the squared error effects to get the Error or Residual sum of squares.

```{r}
sum(err_ss$`E-square`)
```

## Degrees of Freedom
The *df* column above refers to the degrees of freedom.  Remember, the variance is equal to the sum of squares, divided by its degrees of freedom.  The hybrid sum of squares is simply the number of treatments minus 1.  In this example, there were 4 hybrids, so there were three degress of freedom for the Hybrid effect.  The concept behind the hybrid degrees of freedom is that if we know the means for three hybrids, as well as the population mean, then we can calculate the fourth hybrid mean, as it is determined by the first three hybrids and the population mean.  Degrees of freedom are a weird concept, so try not to overanalyze them.

The degrees of freedom for the error or residual effect are a little more confusing.  The degrees of freedom are equal to the Hybrid degrees of freedom, times the number of replications.  In this case, the error degrees of freedom are 12.  The idea behind this is: if for a hybrid you know the values of three observations, plus the hybrid mean, you can calculate the value of the fourth observation.

## Mean Square
In the Analysis of Variance, the Sum of Squares, divided by the degrees of freedom, is referred to as the "Mean Square".  As we now know, the mean squares are also the variances attributable to the Hybrid and Error terms of our linear model.  Our hybrid mean square is about 144.7; the error mean square is about 3.8.

## F-Value
THe F-Value, as introduced earler, is equal to the hybrid variance, divided by the error variance.  In the ANOVA table, F is calculated as the hybrid mean square divided by the error mean square.  When the F-value is 1, it means the treatment effect and error effect have equal variances, and equally describe the variance among observed values.  In other words, knowing the treatment each plot received adds nothing to our understanding of observed differences.


## P-value
The F-value is the summary calculation for the relative sizes of our Hybrid and Error variances.  It's value is 38.4, which means the Hybrid variance is over 38 times the size of the Error variance.  In other words, the Hybrid variance accounts for much, much more of the variation in our observations than the Error variance.  But, given this measured F-value, what is the probability the true F-value is 1, meaning the Hybrid and Error variances are the same?    

To calculate the probability our F-value could be the product of chance, we use the F-distribution.  The shape of the F-distribution, like the t-distribution, changes with the number of replications (which changes the Error degrees of freedom).  It also changes with the treatment degrees of freedom.

Play with the application below.  You are able to adjust the number of treatments and number of replications.  Adjust those two inputs and observe the change in the response curve.  In addition, adjust the desired level of significance and obsere how the shaded area changes.  Please ignore the different color ranges under the curve when you see them: any shaded area under the curve indicates significance.

```{r}

library(sjPlot)
library(shiny)

ui <- fluidPage(
  sidebarLayout(
    sidebarPanel(
      sliderInput("treatmentNo", "Select number of treatments", min=2, max=10, step=1, value=4),
      sliderInput("repNo", "Select number of replications", min=2, max=10, step=1, value=4),
      sliderInput("pLevel", "Select desired level of significance (%)", min = 0.01, max= 0.2, 
                  step = 0.01, value = 0.05)
    ),
    mainPanel(
      plotOutput("plot"),
      textOutput("dfText")
      
    )
  )
)

server <- function(input, output, session) {
  output$plot = renderPlot({
    dist_f(p = input$pLevel, deg.f1 = input$treatmentNo, deg.f2 = input$repNo, xmax = 10)

  })
  
  output$dfText = renderText({
    paste0("Treatment DF: ", input$treatmentNo-1,
           "\nError DF: ", (input$treatmentNo * (input$repNo-1 )))
    
  })
}

shinyApp(ui, server)




```


As you can see, the distribution changes, though not as dramatically, with the number of error degrees of freedom.  In this case, for every 4 degree of freedom increase, the number of replicates increases by one, since the the degrees of freedom is equal to the the number of treatments $\times$ n-1 reps.

The F-distribution is one-tailed -- we are only interested in the proportion remaining in the upper tail.  If we were to visualize the boundary for the areas representing $P\ge0.05$ for our example above, we would test whether F was in the following portion of the tail.

As we can see, our observed F of 38.4 is much greater than what we would need for significance at $P\ge0.05$.  What about $P\ge0.01$?

```{r}
library(sjPlot)

dist_f(p = 0.01, deg.f1 = 3, deg.f2 = 12, xmax = 10)



```

Our value is also way beyond the F-value we would need for $P\ge0.05$.


# Visualizing How the Anova Table Relates to Variance
The app below allows you to simulate a corn trial with three treatments.  You can adjust the three treatment means, as well as the standard deviation within the treatments (the "Pooled Standard Deviation") with the slider controls.  The observed values of the three treatments are represented by dots: treatment 1 (red), treatment 2 (green), and treatment 3 (blue).  Each *treatment* is fit with a distribution curve that is identically colored.  Their widths are based on their variances, which are identical across treatments.  The distribution of *treatment means* is fit with a black curve, which is similarly based on its variance.

The analysis of variance table for the trial, based on the values you select, is shown below.    

Experiment with increasing and decreasing the differences among treatment means.  Observe how increasing and decreasing the differences among treatment means affects the treatment (trt) and residual (error) mean squares.  Also observe how the F-value (labelled "statistic" in the table) and p-value (labelled "p.value") change.

```{r}
library(shiny)
library(broom)

set.seed(1)

ui <- fluidPage(
  sidebarLayout(
    sidebarPanel(
      sliderInput("trtmean1", "Select Treatment Mean", min = 150, max=250, step=1, value=c(184)),
      sliderInput("trtmean2", "Select Treatment Mean", min = 150, max=250, step=1, value=c(185)),
      sliderInput("trtmean3", "Select Treatment Mean", min = 150, max=250, step=1, value=c(189)),
      sliderInput("pooledsd", "Select Pooled Standard Deviation", min = 0, max=30, step=1, value=15)
    ),
    mainPanel(
      plotOutput("plot"),
      tableOutput("anovaTable")
    )
  )
)

server <- function(input, output, session) {
  obsData = reactive({
    set.seed(1)
    rnorm2 <- function(n,mean,sd) { mean+sd*scale(rnorm(n)) }
    trtmean1 = input$trtmean1
    trtmean2 = input$trtmean2
    trtmean3 = input$trtmean3
    sd = input$pooledsd
    trt_1_data = data.frame(trt=rep("trt1", 6))
    trt_1_data$yield = rnorm2(6, mean=trtmean1, sd = sd)
    trt_2_data = data.frame(trt=rep("trt2", 6))
    trt_2_data$yield = rnorm2(6, mean=trtmean2, sd = sd)
    trt_3_data = data.frame(trt=rep("trt3", 6))
    trt_3_data$yield = rnorm2(6, mean=trtmean3, sd = sd)
    obs_data = rbind(trt_1_data, trt_2_data, trt_3_data) 
  })
  
  anovaData = reactive({
    obs_data = obsData()
    anova_data = tidy(aov(yield~trt, data=obs_data))
    })
  
  distCurve = reactive({
    obs_data = obsData()
    anova_data = anovaData()
    root_mse = anova_data %>%
      filter(term=="Residuals") %>%
      mutate(root_mse=sqrt(meansq)) %>%
      select(root_mse) %>%
      as.numeric()
    root_tse = anova_data %>%
      filter(term=="trt") %>%
      mutate(root_mse=sqrt(meansq)) %>%
      select(root_mse) %>%
      as.numeric()
    
    
    within_stats = obs_data %>%
      group_by(trt) %>%
      summarise(mean=mean(yield)) %>%
      mutate(sd = root_mse) %>%
      ungroup()
    
    between_stats = obs_data %>%
      mutate(trt="trt_dist") %>%
      group_by(trt) %>%
      summarise(mean = mean(yield)) %>%
      mutate(sd = root_tse) %>%
      select(trt, mean, sd)
    
    overall_stats = rbind(within_stats, between_stats) %>%
      rename(group = trt)
    
    library(purrr)
    dist_curve = overall_stats %>%
      group_by(group) %>%
      mutate(min = mean - 3.5*sd) %>%
      mutate(max = mean + 3.5*sd) %>%
      mutate(yield = map2(min, max, seq, by=0.1)) %>%
      unnest(yield) %>%
      mutate(prob = scale(dnorm(yield, mean = mean, sd = sd))) %>%
      ungroup()
  })
  
  output$plot = renderPlot({
    obs_data = obsData()
    dist_curve = distCurve()
    within_data = dist_curve %>%
      filter(!group=="trt_dist")
    between_data = dist_curve %>%
      filter(group=="trt_dist")
    ggplot(within_data, aes(x=yield, y=prob)) +
      geom_point(aes(color=group)) +
      geom_point(data = between_data, aes(x=yield, y=prob), color="black") +
      geom_point(data=obs_data, aes(x = yield, y=-0.001, color=trt), size=3) +
      theme(axis.ticks.y = element_blank(),
            axis.text.y = element_blank())
  })
  
  output$anovaTable = renderTable({
    anova_data = anovaData()
  })
   
}

shinyApp(ui, server, options = list(height="450px"))

```

Use your observations to address the following four questions in Discussion 6.1:

1) What do you observe when distance between the trt means increases?
2) what do you observe when the pooled standard deviation decreases?
3) Set the treatment means to treatment 1 = 180, treatment 2 = 188, and treatment 3 = 192.  What do you observe about the shapes of the distribution curve for the treatment means (black curve) and treatment 2 (green curve)?
4) What does an F-value of 1 mean?
