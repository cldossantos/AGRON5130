--- 
title: Exercises -- Linear Contrasts
output: html_document
---


# Introduction
Linear contrasts are different from least significant difference (LSD) or Tukey's honest significant difference (HSD) tests in that they can test groups of treatments.  As we learned in the lecture, the simplest way to think of a linear constrast is as a t-test between two groups.

We also learned in the lecture that we use contrast coefficients to define these groups.  The coefficients for one group should add up to 1; the coefficients of the other group should add up to -1.  That way, we subtract the average value for treatments in one group from the average value for treatments in the other group.

# Case Study: Winter Canola Cultivar Trial.
Six winter canola cultivars were tested in a randomized complete block trial in Manitoba.  Yield is in kg / ha.

```{r}
canola = read.csv("data/canola_gd.csv")
head(canola)

```

## ANOVA
First, let's run our analysis of variance
```{r}
canola_model = aov(yield ~ block + cultivar, data = canola)
summary(canola_model)

```

## Least Significant Difference Test
Next, lets run an LSD test.
```{r}
library(agricolae)
lsd_canola = LSD.test(canola_model, "cultivar")
lsd_canola
```

## Contrasts
Contrasts should be based on specific questions we want to ask of the data.  In this case, there are two  questions we would like to answer with contrasts.

First, cultivars "Donna" and "Bob" are from a line of canola called "PITB".  Sometimes they perform well, but other times their performance can be rather discordant.  Our first question is, do cultivars from the PITB line perform worse than cultivars from other lines.

Cultivars "Mickey" and "Bill" are from the "FOTM" line of cultivars.  They regularly perform better than cultivars with "PITB" pedigrees.  But in bad seasons they have a field performance that has been likened to "sneakers in a drier".  Meanwhile, cultivars "Phil" and "Jerry", from the DKSTR line, under appropriate growing conditions, have a performance potential that is almose unlimited.  Our second question, then, is whether cultivars from the DKSTR outperform those from the FOTM line.

## Figuring Out Which Coefficients to Use
To answer these questions, we need to define our coefficients.  The order of the coefficients must reflect the order of the factor levels in R.  To determine this order we use the *levels()* function.

```{r}
levels(canola$cultivar)
```

What coefficients do we use, then?  Our first hypothesis is that "Bob" and "Donna" perform worse than others.  

1) So we will assign a -1 to both those cultivars. 

Coefficients: 0 -1 -1 0 0 0

2) Since we are comparing against the other four cultivars, we assign them +1:

Coefficients: +1 -1 -1 +1 +1 +1

3) The coefficients of each group must sum to 1.  So we will divide the coefficients for Bob and Donna by 2:

Coefficients: +1 -1/2 -1/2 +1 +1 +1

4) And we will divide the coefficients for the other cultivars by 4:

Coefficients: +1/4 -1/2 -1/2 +1/4 +1/4 +1/4

5) Now, let's check our math.  The coefficients for Bob and Donna should sum to -1

Coefficients: 0 -1/2 -1/2 0 0 0 = -1

6) The coefficients for the other four cultivars should sum to 1.

Coefficients: +1/4 0 0 +1/4 +1/4 +1/4 = 1

7) Finally, all the coefficients together should sum to zero:

Coefficients: +1/4 -1/2 -1/2 +1/4 +1/4 +1/4 = 0


## Telling R the Coefficients to Use
Running the linear coefficient in R requires we define a "coefficient matrix".  A matrix is a slightly more primative version of the data.frame -- still akin to a table, but its rows and columns tend to be defined by numbers instead of names.

To create a coefficient matrix in R, we need to define it as follows:

K = matrix(c(insert coefficients here),1)

"c(coefficients)" tells R to fill in the coefficients across columns.  The ",1" that follows tells R to put those values in the first row. That second part of the matrix, the ",1" is *really important*.  In creating this lesson, I spent at least a half-hour trying to figure out why one of my contrasts would not work.  It was because I had written it:

K_bd_vs_others = matrix(c(+1/4, -1/2, -1/2, +1/4, +1/4, +1/4))

Instead of:

K_bd_vs_others = matrix(c(+1/4, -1/2, -1/2, +1/4, +1/4, +1/4), 1)

```{r}
K_bd_vs_others = matrix(c(+1/4, -1/2, -1/2, +1/4, +1/4, +1/4),1)

K_bd_vs_others
```

Above is our coefficient matrix for "Bob" and "Donna" versus the others.  

Now let's create our second matrix, for "Mickey" and "Bill" versus "Phil" and "Jerry".  Let's check the order of the factor levels again.

```{r}
levels(canola$cultivar)
```

Our hypothesis is that "Phil" and "Jerry" perform better than "Mickey" and "Bill", so we will assign them the initial coefficients of 1 

1) So we will assign a -1 to both those cultivars. 

Coefficients: 0 0 0 1 0 1

2) We then assign Mickey and Bill the coefficients -1:

Coefficients: -1 0 0 +1 -1 +1

"Donna" and "Bob" are assigned the coefficient zero since they are not involved in this comparison.

3) The coefficients of each group must sum to 1.  So we will divide the coefficients for "Jerry" and "Phil" by 2:

Coefficients: -1 0 0 +1/2 -1 +1/2

4) We do the same with "Mickey" and "Bill":

Coefficients: -1/2 0 0 +1/2 -1/2 +1/2

5) Now, let's check our math.  The coefficients for Jerry and Phil should sum to 1

Coefficients: -0 0 0 +1/2 0 +1/2 = 1

6) The coefficients for Mickey and Bill should sum to -1.

Coefficients: -1/2 0 0 0 -1/2 0 = -1

Finally, all the coefficients together should sum to zero:

Coefficients: -1/2 0 0 +1/2 -1/2 +1/2 = 0

Our correlation coefficient for our second contrast, then, is:

```{r}
K_jp_vs_mb = matrix(c(-1/2, 0, 0, +1/2, -1/2, +1/2),1)
K_jp_vs_mb
```


# Running the Contrast
Defining the contrast coefficients is difficult. Running the contrast afterwards is relatively easy.  We use the *ghlt()* function from the *multicomp()* package.  "glht" stands for "generalized linear hypothesis test". 

*glht()* requires two arguments: a linear model, and the coefficient contrast we created above.  The linear model is a little different than what we defined for the ANOVA above.  It only contains the terms "0" and the treatment name:

```{r}
canola_contrast_model = aov(yield ~ 0 + cultivar, data=canola)
```

The reason for this is because our contrast is isolating the effect of cultivar.  The zero forces our contrast to calculate the actual difference between the two groups' means (which we may like to report).

Now we can run our contrast
```{r}
library(multcomp)

bd_vs_others = glht(canola_contrast_model, linfct = K_bd_vs_others)
```

We will use *summary()* to summarise these results.

```{r}
summary(bd_vs_others)
```

Ok, let's review our results.  The Estimate, 820.2 kg / ha, is the difference between the group means.  Since we subracted the mean of Bob and Donna from the mean of the other four cultivars, we concluded that Bob and Donna as a group yielded *less* than the other four cultivars as a group.  

The Estimate was divided by Std. Error, the standard error of the difference, to produce the t value of 5.872.  The probability of observing a t-value of this size or greater, if in fact the true difference between groups was zero, is very small, 1.47 x 10^-5.  The difference between groups is significant.

We conclude that cultivars from the PITB line yielded worse than other cultivars as a group.

We can now run the second contrast, Jerry and Phil versus Mickey and Bill.  We again use the canola_contrast_model as our linear model, and this time, the coefficient matrix K_jp_vs_mb

```{r}
jp_vs_mb = glht(canola_contrast_model, linfct=K_jp_vs_mb)
summary(jp_vs_mb)

```

The mean of Jerry and Phill was 438.0 kg/ha greater than the mean of Mickey and Bill.  (Note: when you look at the Estimate, always ask yourself whether the number, based on treatment means, makes sense.  The first time I ran this contrast, for example, the estimate was greater than 2000.  This caused me to review my contrast coefficients, where I discovered one of the coefficients was positive when it should have been negative.)

Our t value again has a probability of occuring by chance of less than P=0.05, so the difference between the mean of Jerry and the mean of Phil and Mickey and Bill is significant.  We conclude that cultivars from the DKSTR line yielded better than cultivars from the FOTM line.



# Practice: Corn Nitrogen Source and Timing
This is a really powerful example of how contrasts can be used, not just to test significances, but generate much broader understandings than either the ANOVA or LSD/HSD alone would allow.  Our practice data set is from a nitrogen management trial was conducted near Whitehouse, Ohio, to compare 7 nitrogen management strategies in corn.  All treatments (other than the control) provided 200 units (lbs) total N during the growing season.

```{r}
n_source_timing = read.csv("data/corn_nitrogen_source_timing.csv")
head(n_source_timing)

```

Lets look at the seven levels of nitrogen treatment:

```{r}
levels(n_source_timing$trt)
```

Treatment abbreviations are: "aa" = anhydrous amonia, "uan" = urea ammonium nitrate, "pre" = pre-plant, and "post" = post-emergence (sidedress).

## ANOVA

First, let's look at our analysis of variance:
```{r}
corn_nitrogen_model  = aov(yield ~ block + trt, data=n_source_timing)
summary(corn_nitrogen_model)
```

Our nitrogen treatments are significantly different.  What does the least significant test tell us differences among treatment means?

```{r}
lsd = LSD.test(corn_nitrogen_model, "trt")
lsd$groups
```

We have four questions:
1) is the mean yield of the six nitrogen treatments significantly different from the untreated control?

2) is the mean yield of preplant nitrogen treatments significantly different from the mean of postemergence nitrogen treatments?

3) is the mean yield of anhydrous ammonia treatments significantly different from the mean of urea ammonium nitrate treatments? 

4) is the mean yield of split application treatments significantly different from the mean of single application treatments? 

## Nitrogen Treatments vs Control
Lets walk through this one together.  What contrast coefficients do we use?  Lets first double-check the order of factor levels (treatments):

```{r}
levels(n_source_timing$trt)
```

So we will assign -1 to the control treatment and 1 to all other treatments:

coefficients: 1 1 1 -1 1 1 1

The coefficients assigned to each group should add up to an absolute value.  Since there are six nitrogen treatments, we need to divide each of their coefficients by six.

coefficients: 1/6 1/6 1/6 -1 1/6 1/6 1/6

Now, if we check, we will see the coefficients assigned to the six nitrogen treatments sum to 1, the control coefficient is zero, and the sum of all coefficients is 0.

Now let's define our first contrast coefficient matrix by filling in the matrix below:

```{r}
K_n_vs_control = matrix(c(1/6, 1/6, 1/6, -1, 1/6, 1/6, 1/6), 1)  # don't forget the ", 1" after the coefficients!
```

We need to define our nitrogen contrast model.  Remember, the model only has 0 and the factor name on the right side of the equation.  Complete our equation below.

```{r}
n_contrast_model = aov(yield ~ 0 + trt, data=n_source_timing)
```

Finally, run the contrast using the glht() and summary() functions below:

```{r}
n_vs_control = glht(n_contrast_model, linfct=K_n_vs_control)
summary(n_vs_control)

```

What do you conclude?

## Preplant vs Postemergence Treatments
Compare the two pre-plant treatments ("_pre") to the two post-emergence ("_post") treatments.  Your results should look like:


	 Simultaneous Tests for General Linear Hypotheses

Fit: aov(formula = yield ~ 0 + trt, data = n_source_timing)

Linear Hypotheses:
       Estimate Std. Error t value Pr(>|t|)  
1 == 0   -7.462      3.427  -2.178    0.041 *
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
(Adjusted p values reported -- single-step method)

What do you conclude?

## Anhydrous Ammonian vs Urea Ammonium Nitrate
Compare the three anhydrous ammonia ("aa_") treatments to the three urea ammonium nitrate ("_uan") treatments.  Your results should look like:


	 Simultaneous Tests for General Linear Hypotheses

Fit: aov(formula = yield ~ 0 + trt, data = n_source_timing)

Linear Hypotheses:
       Estimate Std. Error t value Pr(>|t|)   
1 == 0    8.193      2.798   2.929  0.00803 **
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
(Adjusted p values reported -- single-step method)

What do you conclude?

## Split vs Single Applications
Compare the two split application ("_pre_post") treatments to the four single-application ("_pre" or "_post") treatments.  Your results should look like:


	 Simultaneous Tests for General Linear Hypotheses

Fit: aov(formula = yield ~ 0 + trt, data = n_source_timing)

Linear Hypotheses:
       Estimate Std. Error t value Pr(>|t|)   
1 == 0   10.220      2.967   3.444  0.00243 **
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
(Adjusted p values reported -- single-step method)

What do you conclude?